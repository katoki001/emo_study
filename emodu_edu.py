# -*- coding: utf-8 -*-
"""emodu_edu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EaymXzLcJ1pLGuLRLZAkhduJk2ya0mWl
"""

!pip install faiss-cpu

import numpy as np
import torch
from sentence_transformers import SentenceTransformer

def generate_embeddings():
    df = pd.read_csv("physics_clean_dataset.csv", usecols=["text_content"])
    df = df.head(3000)
    sentences = df["text_content"].tolist()

    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device="cpu")
    torch.set_num_threads(4)

    embeddings = model.encode(
        sentences,
        batch_size=128,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=True
    )

    np.save("light_embeddings.npy", embeddings)
    df.to_csv("physics_texts.csv", index=False)
    print("Saved embeddings + texts")
    print("Shape:", embeddings.shape)

generate_embeddings()

import faiss
import numpy as np

emb = np.load("light_embeddings.npy").astype("float32")

index = faiss.IndexFlatIP(emb.shape[1])
index.add(emb)
faiss.write_index(index, "physics_index.faiss")

print("FAISS index saved")

import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

texts = pd.read_csv("physics_texts.csv")["text_content"].tolist()
index = faiss.read_index("physics_index.faiss")
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def retrieve(query, k=5):
    q_emb = model.encode([query]).astype("float32")
    D, I = index.search(q_emb, k)
    return [texts[i] for i in I[0]]

def answer_question(question):
    chunks = retrieve(question, 5)
    combined = "\n---\n".join(chunks)
    return f"Relevant physics information:\n\n{combined}"

# =======================
# Imports
# =======================
import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import gc


# =======================
# Load data & models
# =======================
texts = pd.read_csv("physics_texts.csv")["text_content"].tolist()
index = faiss.read_index("physics_index.faiss")

embed_model = SentenceTransformer(
    "sentence-transformers/all-MiniLM-L6-v2",
    device="cpu"
)

llm_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(llm_name)

llm = AutoModelForCausalLM.from_pretrained(
    llm_name,
    device_map={"": "cpu"},
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True
)


# =======================
# Helper functions
# =======================
def is_numerical_problem(question: str) -> bool:
    keywords = [
        "calculate", "determine", "find",
        "velocity", "acceleration",
        "distance", "time", "speed"
    ]
    return any(k in question.lower() for k in keywords)

def retrieve(query, k=2):
    q_emb = embed_model.encode([query]).astype("float32")
    _, I = index.search(q_emb, k)

    context_chunks = []
    links = []

    for i in I[0]:
        text = texts[i]
        # If the text contains your custom YouTube separator, treat it as a link
        if " â€” http" in text:
            links.append(text)
        else:
            context_chunks.append(text[:600])

    return context_chunks, links


# =======================
# Core generation
# =======================
def generate_explanation(question, context, links):
    q_low = question.lower()

    # Mode 1: Schedule & Verification
    if any(w in q_low for w in ["schedule", "plan", "test me", "verify", "3 day"]):
        system_prompt = """You are a physics tutor.
        1. Ask the user 2 concept questions to verify their knowledge.
        2. Provide a 3-day study schedule based on the context.
        Format:
        ### 1. Verification Questions
        ### 2. 3-Day Schedule"""

    # Mode 2: Solving
    elif is_numerical_problem(question):
        system_prompt = """You are a strict Physics Tutor for beginners.
1. Use ONLY: Work = Force * Distance.
2. Units: Force in Newtons (N), Distance in Meters (m), Work in Joules (J).
3. DO NOT use time (s) in your calculation.
4. Calculate carefully:
5. Provide ONLY the requested sections.
        Solve the problem using this format.
        ### 1. Given
        ### 2. Formula
        ### 3. Solution
        ### 4. Final Answer """
    else:
        system_prompt = "You are a physics tutor. Provide a detailed Explanation and 4-7 bulleted Key Points."

    # Prepare messages for Chat Template
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion:\n{question}"}
    ]

    # Apply template and tokenize
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to("cpu") # Ensure it's on CPU as per your setup

    # GENERATION FIX: Added do_sample=True and pad_token_id
    outputs = llm.generate(
        **inputs,
        max_new_tokens=450,
        temperature=0.2,
        do_sample=True, # Required to use temperature
        pad_token_id=tokenizer.eos_token_id
    )

    # Decode only the new tokens
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)

    # Mode 3: Add Links (Read/Watch section)
    link_section = ""
    if links:
        link_section = "\n\n### 3. Study Resources (Watch & Read)\n" + "\n".join([f"â€¢ {l}" for l in links])

    return response.strip() + link_section


def answer_question(question: str):
    chunks, links = retrieve(question, 5) # Increased k to find both text and links
    context = "\n\n".join(chunks)
    return generate_explanation(question, context, links)

import torch
import gc

def clean_memory():
    torch.cuda.empty_cache()
    gc.collect()

print(answer_question(input()))

clean_memory()

# Launch with public URL
import gradio as gr  # <-- THIS WAS MISSING
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
def api_handler(prompt):
    try:
        result = generate_text(prompt)
        return result
    except Exception as e:
        return f"Error: {str(e)}"

demo = gr.Interface(
    fn=api_handler,
    inputs=gr.Textbox(label="Your Prompt"),
    outputs=gr.Textbox(label="LLM Response"),
    title="My LLM API",
    description="Send prompts via HTTP to this endpoint"
)

# THIS CREATES THE PUBLIC URL
demo.launch(share=True, debug=True)

!pip install -q gradio transformers torch accelerate

"""# ***TG***"""

from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    MessageHandler,
    ContextTypes,
    filters
)

BOT_TOKEN = "8587589105:AAGJ4eIEwKm5JQ_NiS0GRCF5pv2gb6Q9aM4"


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_text = update.message.text
    if not user_text:
        return

    await update.message.reply_text("â³ Thinking...")

    try:
        answer = answer_question(user_text)
        await update.message.reply_text(answer)
    except Exception as e:
        await update.message.reply_text("âš ï¸ Error occurred.")
        print(e)

    clean_memory()


async def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(
        MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)
    )
    print("ðŸ¤– Physics bot is running...")
    await app.run_polling()


import nest_asyncio
nest_asyncio.apply()

await main()

!pip install -U python-telegram-bot==20.7

!pip install nest_asyncio
import nest_asyncio

nest_asyncio.apply()

app = ApplicationBuilder().token(BOT_TOKEN).build()
app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

print("Bot is running...")
app.run_polling()