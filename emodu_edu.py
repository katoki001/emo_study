# -*- coding: utf-8 -*-
"""emodu_edu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EaymXzLcJ1pLGuLRLZAkhduJk2ya0mWl
"""

!pip install faiss-cpu

"""Imports"""

#used probobly everywhere
import numpy as np
import torch
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc
import json

#for documents
import random
import threading
import fitz
from docx import Document as DocxDocument
import textwrap

"""Embedding generating"""

def generate_embeddings():
    df = pd.read_csv("physics_clean_categorized.csv", usecols=["text_content"])
    df = df.head(3000)
    sentences = df["text_content"].tolist()

    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device="cpu")
    torch.set_num_threads(4)te

    embeddings = model.encode(
        sentences,
        batch_size=128,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=True
    )

    np.save("light_embeddings.npy", embeddings)
    df.to_csv("physics_texts.csv", index=False)
    print("Saved embeddings + texts")
    print("Shape:", embeddings.shape)

generate_embeddings()

"""Clustering embeddings better with faiss"""

emb = np.load("light_embeddings.npy").astype("float32")

index = faiss.IndexFlatIP(emb.shape[1])
index.add(emb)
faiss.write_index(index, "physics_index.faiss")

print("FAISS index saved")

texts = pd.read_csv("physics_texts.csv")["text_content"].tolist()
index = faiss.read_index("physics_index.faiss")
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def retrieve(query, k=5):
    q_emb = model.encode([query]).astype("float32")
    D, I = index.search(q_emb, k)
    return [texts[i] for i in I[0]]

def answer_question(question):
    chunks = retrieve(question, 5)
    combined = "\n---\n".join(chunks)
    return f"Relevant physics information:\n\n{combined}"

"""LLM part"""

texts = pd.read_csv("physics_texts.csv")["text_content"].tolist()
index = faiss.read_index("physics_index.faiss")

embed_model = SentenceTransformer(
    "sentence-transformers/all-MiniLM-L6-v2",
    device="cpu"
)

llm_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(llm_name)

llm = AutoModelForCausalLM.from_pretrained(
    llm_name,
    device_map={"": "cpu"},
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True
)

def retrieve(query, k=5):
    q_emb = embed_model.encode([query]).astype("float32")
    _, I = index.search(q_emb, k)
    chunks = []

#it's an numerical problem
def is_numerical_problem(question: str) -> bool:
    keywords = [
        "calculate", "determine", "find",
        "velocity", "acceleration",
        "distance", "time", "speed"
    ]
    return any(k in question.lower() for k in keywords)

#question answering
def answer_question(question: str):
    chunks, links = retrieve(question, 5)
    context = "\n\n".join(chunks)

    #numerical
    if is_numerical_problem(question):
        system_prompt = "You are a strict Physics Tutor. Format: ### 1. Given, ### 2. Formula, ### 3. Solution, ### 4. Final Answer."
    #ususal theory question
    else:
        system_prompt = "You are a physics tutor. Provide a detailed Explanation and 4-7 bulleted Key Points."

    messages = [{"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Context:\n{context}\n\nQuestion:\n{question}"}]
    #tokenisation and answer generating
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
    outputs = llm.generate(**inputs, max_new_tokens=450, temperature=0.2, do_sample=True, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)

#fleshcards, for studing
def generate_flashcards_from_text(text: str, num_cards: int = 5) -> list[dict]:
    messages = [
        {"role": "system", "content": 'Return ONLY a raw JSON array of objects with "question" and "answer" keys.'},
        {"role": "user", "content": f"Create {num_cards} flashcards from this text:\n\n{text[:2000]}"}
    ]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
    outputs = llm.generate(**inputs, max_new_tokens=600, temperature=0.3, do_sample=True, pad_token_id=tokenizer.eos_token_id)
    raw = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True).strip()
    try:
        start, end = raw.index("["), raw.rindex("]") + 1
        return json.loads(raw[start:end])
    except:
        return [{"question": "Error", "answer": "Could not generate flashcards."}]

"""new feature document reading and summrising"""

#getting and reading document
def read_document(file_path: str) -> str:
    ext = file_path.lower().split(".")[-1]
    if ext == "pdf":
        doc = fitz.open(file_path)
        text = "".join(page.get_text() for page in doc)
        doc.close()
        return text.strip()
    elif ext == "docx":
        doc = DocxDocument(file_path)
        return "\n".join(para.text for para in doc.paragraphs if para.text.strip())
    elif ext in ("txt", "md"):
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read()
    else:
        raise ValueError(f"Unsupported file type: .{ext}")

#splits text into chunks to tokenize easier
def chunk_text(text: str, max_chars: int = 2500) -> list[str]:
    sentences = text.replace("\n", " ").split(". ")
    chunks, current = [], ""
    for sent in sentences:
        if len(current) + len(sent) < max_chars:
            current += sent + ". "
        else:
            if current: chunks.append(current.strip())
            current = sent + ". "
    if current: chunks.append(current.strip())
    return chunks

#summarization of chunck
def summarize_chunk(chunk: str) -> str:
    messages = [
        {"role": "system", "content": "You are an expert academic summarizer. Use plain prose."},
        {"role": "user", "content": f"Write a focused 3-5 sentence summary of this passage:\n\n{chunk}"}
    ]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
    outputs = llm.generate(**inputs, max_new_tokens=200, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True).strip()


#full summarization
def summarize_document(file_path: str) -> str:
    raw_text = read_document(file_path)
    chunks = chunk_text(raw_text)
    chunk_summaries = [summarize_chunk(c) for c in chunks]

    if len(chunk_summaries) > 1:
        combined = "\n\n".join(chunk_summaries)
        messages = [
            {"role": "system", "content": "Combine these partial summaries into one coherent final summary."},
            {"role": "user", "content": f"Summaries:\n{combined}"}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
        outputs = llm.generate(**inputs, max_new_tokens=350, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)
        return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True).strip()
    return chunk_summaries[0]

def clean_memory():
    torch.cuda.empty_cache()
    gc.collect()
clean_memory()

from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok, conf

app = Flask(__name__)
CORS(app)

@app.route('/chat', methods=['POST'])
def api_chat():
    msg = request.json.get('message', '')
    return jsonify({'response': answer_question(msg)})

@app.route('/summarize', methods=['POST'])
def api_summarize():
    path = request.json.get('file_path', '')
    if not os.path.exists(path): return jsonify({'error': 'File not found'}), 404
    return jsonify({'summary': summarize_document(path)})

@app.route('/study', methods=['POST'])
def api_study():
    source = request.json.get('source', '')
    num = request.json.get('num_cards', 5)
    if os.path.isfile(source):
        text = read_document(source)
    else:
        chunks, _ = retrieve(source, k=3)
        text = "\n".join(chunks)
    return jsonify({'flashcards': generate_flashcards_from_text(text, num)})

@app.route('/health', methods=['GET'])
def health(): return jsonify({'status': 'ok'})

# ============================================================
# 6. RUN SERVER
# ============================================================
!kill -9 $(lsof -t -i:5000) 2>/dev/null
public_url = ngrok.connect(5000)
print(f"\nâœ… PHYSICS API LIVE: {public_url}")

threading.Thread(target=lambda: app.run(port=5000, use_reloader=False, host='0.0.0.0')).start()